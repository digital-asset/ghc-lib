diff --git a/compiler/GHC/Driver/Make.hs b/compiler/GHC/Driver/Make.hs
index 08d89aede9..d72b452d2e 100644
--- a/compiler/GHC/Driver/Make.hs
+++ b/compiler/GHC/Driver/Make.hs
@@ -27,7 +27,7 @@
 -- -----------------------------------------------------------------------------
 module GHC.Driver.Make (
         depanal, depanalE, depanalPartial, checkHomeUnitsClosed,
-        load, loadWithCache, load', AnyGhcDiagnostic, LoadHowMuch(..), ModIfaceCache(..), noIfaceCache, newIfaceCache,
+        load, loadWithCache, load', LoadHowMuch(..), ModIfaceCache(..), noIfaceCache, newIfaceCache,
         instantiationNodes,
 
         downsweep,
@@ -75,7 +75,6 @@ import GHC.Driver.Env
 import GHC.Driver.Errors
 import GHC.Driver.Errors.Types
 import GHC.Driver.Main
-import GHC.Driver.MakeSem
 
 import GHC.Parser.Header
 
@@ -152,10 +151,10 @@ import GHC.Runtime.Loader
 import GHC.Rename.Names
 import GHC.Utils.Constants
 import GHC.Types.Unique.DFM (udfmRestrictKeysSet)
+import qualified Data.IntSet as I
 import GHC.Types.Unique
 import GHC.Iface.Errors.Types
 
-import qualified Data.IntSet as I
 
 -- -----------------------------------------------------------------------------
 -- Loading the program
@@ -486,7 +485,7 @@ newIfaceCache = do
 -- All other errors are reported using the 'defaultWarnErrLogger'.
 
 load :: GhcMonad f => LoadHowMuch -> f SuccessFlag
-load how_much = loadWithCache noIfaceCache mkUnknownDiagnostic how_much
+load how_much = loadWithCache noIfaceCache how_much
 
 mkBatchMsg :: HscEnv -> Messager
 mkBatchMsg hsc_env =
@@ -495,18 +494,12 @@ mkBatchMsg hsc_env =
     then batchMultiMsg
     else batchMsg
 
-type AnyGhcDiagnostic = UnknownDiagnostic (DiagnosticOpts GhcMessage)
 
-loadWithCache :: GhcMonad m => Maybe ModIfaceCache -- ^ Instructions about how to cache interfaces as we create them.
-                            -> (GhcMessage -> AnyGhcDiagnostic) -- ^ How to wrap error messages before they are displayed to a user.
-                                                                -- If you are using the GHC API you can use this to override how messages
-                                                                -- created during 'loadWithCache' are displayed to the user.
-                            -> LoadHowMuch -- ^ How much `loadWithCache` should load
-                            -> m SuccessFlag
-loadWithCache cache diag_wrapper how_much = do
+loadWithCache :: GhcMonad m => Maybe ModIfaceCache -> LoadHowMuch -> m SuccessFlag
+loadWithCache cache how_much = do
     (errs, mod_graph) <- depanalE [] False                        -- #17459
     msg <- mkBatchMsg <$> getSession
-    success <- load' cache how_much diag_wrapper (Just msg) mod_graph
+    success <- load' cache how_much (Just msg) mod_graph
     if isEmptyMessages errs
       then pure success
       else throwErrors (fmap GhcDriverMessage errs)
@@ -671,39 +664,11 @@ createBuildPlan mod_graph maybe_top_mod =
               (vcat [text "Build plan missing nodes:", (text "PLAN:" <+> ppr (sum (map countMods build_plan))), (text "GRAPH:" <+> ppr (length (mgModSummaries' mod_graph )))])
               build_plan
 
-mkWorkerLimit :: DynFlags -> IO WorkerLimit
-mkWorkerLimit dflags =
-  case parMakeCount dflags of
-    Nothing -> pure $ num_procs 1
-    Just (ParMakeSemaphore h) -> pure (JSemLimit (SemaphoreName h))
-    Just ParMakeNumProcessors -> num_procs <$> getNumProcessors
-    Just (ParMakeThisMany n) -> pure $ num_procs n
-  where
-    num_procs x = NumProcessorsLimit (max 1 x)
-
-isWorkerLimitSequential :: WorkerLimit -> Bool
-isWorkerLimitSequential (NumProcessorsLimit x) = x <= 1
-isWorkerLimitSequential (JSemLimit {})         = False
-
--- | This describes what we use to limit the number of jobs, either we limit it
--- ourselves to a specific number or we have an external parallelism semaphore
--- limit it for us.
-data WorkerLimit
-  = NumProcessorsLimit Int
-  | JSemLimit
-    SemaphoreName
-      -- ^ Semaphore name to use
-  deriving Eq
-
 -- | Generalized version of 'load' which also supports a custom
 -- 'Messager' (for reporting progress) and 'ModuleGraph' (generally
 -- produced by calling 'depanal'.
-load' :: GhcMonad m => Maybe ModIfaceCache -> LoadHowMuch -> (GhcMessage -> AnyGhcDiagnostic) -> Maybe Messager -> ModuleGraph -> m SuccessFlag
-load' mhmi_cache how_much diag_wrapper mHscMessage mod_graph = do
-    -- In normal usage plugins are initialised already by ghc/Main.hs this is protective
-    -- for any client who might interact with GHC via load'.
-    -- See Note [Timing of plugin initialization]
-    initializeSessionPlugins
+load' :: GhcMonad m => Maybe ModIfaceCache -> LoadHowMuch -> Maybe Messager -> ModuleGraph -> m SuccessFlag
+load' mhmi_cache how_much mHscMessage mod_graph = do
     modifySession $ \hsc_env -> hsc_env { hsc_mod_graph = mod_graph }
     guessOutputFile
     hsc_env <- getSession
@@ -779,17 +744,21 @@ load' mhmi_cache how_much diag_wrapper mHscMessage mod_graph = do
     liftIO $ debugTraceMsg logger 2 (hang (text "Ready for upsweep")
                                     2 (ppr build_plan))
 
-    worker_limit <- liftIO $ mkWorkerLimit dflags
+    n_jobs <- case parMakeCount (hsc_dflags hsc_env) of
+                    Nothing -> liftIO getNumProcessors
+                    Just n  -> return n
 
     setSession $ hscUpdateHUG (unitEnv_map pruneHomeUnitEnv) hsc_env
-    (upsweep_ok, new_deps) <- withDeferredDiagnostics $ do
+    (upsweep_ok, hsc_env1) <- withDeferredDiagnostics $ do
       hsc_env <- getSession
-      liftIO $ upsweep worker_limit hsc_env mhmi_cache diag_wrapper mHscMessage (toCache pruned_cache) build_plan
-    modifySession (addDepsToHscEnv new_deps)
+      liftIO $ upsweep n_jobs hsc_env mhmi_cache mHscMessage (toCache pruned_cache) build_plan
+    setSession hsc_env1
     case upsweep_ok of
       Failed -> loadFinish upsweep_ok
       Succeeded -> do
           liftIO $ debugTraceMsg logger 2 (text "Upsweep completely successful.")
+          -- Clean up after ourselves
+          liftIO $ cleanCurrentModuleTempFilesMaybe logger (hsc_tmpfs hsc_env1) dflags
           loadFinish upsweep_ok
 
 
@@ -1067,7 +1036,13 @@ getDependencies direct_deps build_map =
 type BuildM a = StateT BuildLoopState IO a
 
 
+-- | Abstraction over the operations of a semaphore which allows usage with the
+--  -j1 case
+data AbstractSem = AbstractSem { acquireSem :: IO ()
+                               , releaseSem :: IO () }
 
+withAbstractSem :: AbstractSem -> IO b -> IO b
+withAbstractSem sem = MC.bracket_ (acquireSem sem) (releaseSem sem)
 
 -- | Environment used when compiling a module
 data MakeEnv = MakeEnv { hsc_env :: !HscEnv -- The basic HscEnv which will be augmented for each module
@@ -1078,7 +1053,6 @@ data MakeEnv = MakeEnv { hsc_env :: !HscEnv -- The basic HscEnv which will be au
                        --          into the log queue.
                        , withLogger :: forall a . Int -> ((Logger -> Logger) -> IO a) -> IO a
                        , env_messager :: !(Maybe Messager)
-                       , diag_wrapper :: GhcMessage -> AnyGhcDiagnostic
                        }
 
 type RunMakeM a = ReaderT MakeEnv (MaybeT IO) a
@@ -1253,20 +1227,20 @@ withCurrentUnit uid = do
   local (\env -> env { hsc_env = hscSetActiveUnitId uid (hsc_env env)})
 
 upsweep
-    :: WorkerLimit -- ^ The number of workers we wish to run in parallel
+    :: Int -- ^ The number of workers we wish to run in parallel
     -> HscEnv -- ^ The base HscEnv, which is augmented for each module
     -> Maybe ModIfaceCache -- ^ A cache to incrementally write final interface files to
-    -> (GhcMessage -> AnyGhcDiagnostic)
     -> Maybe Messager
     -> M.Map ModNodeKeyWithUid HomeModInfo
     -> [BuildPlan]
-    -> IO (SuccessFlag, [HomeModInfo])
-upsweep n_jobs hsc_env hmi_cache diag_wrapper mHscMessage old_hpt build_plan = do
+    -> IO (SuccessFlag, HscEnv)
+upsweep n_jobs hsc_env hmi_cache mHscMessage old_hpt build_plan = do
     (cycle, pipelines, collect_result) <- interpretBuildPlan (hsc_HUG hsc_env) hmi_cache old_hpt build_plan
-    runPipelines n_jobs hsc_env diag_wrapper mHscMessage pipelines
+    runPipelines n_jobs hsc_env mHscMessage pipelines
     res <- collect_result
 
     let completed = [m | Just (Just m) <- res]
+    let hsc_env' = addDepsToHscEnv completed hsc_env
 
     -- Handle any cycle in the original compilation graph and return the result
     -- of the upsweep.
@@ -1274,10 +1248,10 @@ upsweep n_jobs hsc_env hmi_cache diag_wrapper mHscMessage old_hpt build_plan = d
         Just mss -> do
           let logger = hsc_logger hsc_env
           liftIO $ fatalErrorMsg logger (cyclicModuleErr mss)
-          return (Failed, [])
+          return (Failed, hsc_env)
         Nothing  -> do
           let success_flag = successIf (all isJust res)
-          return (success_flag, completed)
+          return (success_flag, hsc_env')
 
 toCache :: [HomeModInfo] -> M.Map (ModNodeKeyWithUid) HomeModInfo
 toCache hmis = M.fromList ([(miKey $ hm_iface hmi, hmi) | hmi <- hmis])
@@ -1686,10 +1660,8 @@ downsweep hsc_env old_summaries excl_mods allow_dup_roots
             k = NodeKey_Module (msKey ms)
 
             hs_file_for_boot
-              | HsBootFile <- ms_hsc_src ms
-              = Just $ ((ms_unitid ms), NoPkgQual, (GWIB (noLoc $ ms_mod_name ms) NotBoot))
-              | otherwise
-              = Nothing
+              | HsBootFile <- ms_hsc_src ms = Just $ ((ms_unitid ms), NoPkgQual, (GWIB (noLoc $ ms_mod_name ms) NotBoot))
+              | otherwise = Nothing
 
 
         -- This loops over each import in each summary. It is mutually recursive with loopSummaries if we discover
@@ -2214,9 +2186,9 @@ summariseModule hsc_env' home_unit old_summary_map is_boot (L _ wanted_mod) mb_p
         -- annotation, but we don't know if it's a signature or a regular
         -- module until we actually look it up on the filesystem.
         let hsc_src
-              | is_boot == IsBoot           = HsBootFile
+              | is_boot == IsBoot = HsBootFile
               | isHaskellSigFilename src_fn = HsigFile
-              | otherwise                   = HsSrcFile
+              | otherwise = HsSrcFile
 
         when (pi_mod_name /= wanted_mod) $
                 throwE $ singleMessage $ mkPlainErrorMsgEnvelope pi_mod_name_loc
@@ -2342,21 +2314,18 @@ withDeferredDiagnostics f = do
           let action = logMsg logger msgClass srcSpan msg
           case msgClass of
             MCDiagnostic SevWarning _reason _code
-              -> atomicModifyIORef' warnings $ \(!i) -> (action: i, ())
+              -> atomicModifyIORef' warnings $ \i -> (action: i, ())
             MCDiagnostic SevError _reason _code
-              -> atomicModifyIORef' errors   $ \(!i) -> (action: i, ())
+              -> atomicModifyIORef' errors   $ \i -> (action: i, ())
             MCFatal
-              -> atomicModifyIORef' fatals   $ \(!i) -> (action: i, ())
+              -> atomicModifyIORef' fatals   $ \i -> (action: i, ())
             _ -> action
 
         printDeferredDiagnostics = liftIO $
           forM_ [warnings, errors, fatals] $ \ref -> do
             -- This IORef can leak when the dflags leaks, so let us always
-            -- reset the content. The lazy variant is used here as we want to force
-            -- this error if the IORef is ever accessed again, rather than now.
-            -- See #20981 for an issue which discusses this general issue.
-            let landmine = if debugIsOn then panic "withDeferredDiagnostics: use after free" else []
-            actions <- atomicModifyIORef ref $ \i -> (landmine, i)
+            -- reset the content.
+            actions <- atomicModifyIORef' ref $ \i -> ([], i)
             sequence_ $ reverse actions
 
     MC.bracket
@@ -2432,9 +2401,8 @@ cyclicModuleErr mss
 
 cleanCurrentModuleTempFilesMaybe :: MonadIO m => Logger -> TmpFs -> DynFlags -> m ()
 cleanCurrentModuleTempFilesMaybe logger tmpfs dflags =
-  if gopt Opt_KeepTmpFiles dflags
-    then liftIO $ keepCurrentModuleTempFiles logger tmpfs
-    else liftIO $ cleanCurrentModuleTempFiles logger tmpfs
+  unless (gopt Opt_KeepTmpFiles dflags) $
+    liftIO $ cleanCurrentModuleTempFiles logger tmpfs
 
 
 addDepsToHscEnv ::  [HomeModInfo] -> HscEnv -> HscEnv
@@ -2450,12 +2418,12 @@ setHUG deps hsc_env =
   hscUpdateHUG (const $ deps) hsc_env
 
 -- | Wrap an action to catch and handle exceptions.
-wrapAction :: (GhcMessage -> AnyGhcDiagnostic) -> HscEnv -> IO a -> IO (Maybe a)
-wrapAction msg_wrapper hsc_env k = do
+wrapAction :: HscEnv -> IO a -> IO (Maybe a)
+wrapAction hsc_env k = do
   let lcl_logger = hsc_logger hsc_env
       lcl_dynflags = hsc_dflags hsc_env
       print_config = initPrintConfig lcl_dynflags
-  let logg err = printMessages lcl_logger print_config (initDiagOpts lcl_dynflags) (msg_wrapper <$> srcErrorMessages err)
+  let logg err = printMessages lcl_logger print_config (initDiagOpts lcl_dynflags) (srcErrorMessages err)
   -- MP: It is a bit strange how prettyPrintGhcErrors handles some errors but then we handle
   -- SourceError and ThreadKilled differently directly below. TODO: Refactor to use `catches`
   -- directly. MP should probably use safeTry here to not catch async exceptions but that will regress performance due to
@@ -2505,10 +2473,9 @@ executeInstantiationNode k n deps uid iu = do
         -- Output of the logger is mediated by a central worker to
         -- avoid output interleaving
         msg <- asks env_messager
-        wrapper <- asks diag_wrapper
         lift $ MaybeT $ withLoggerHsc k env $ \hsc_env ->
           let lcl_hsc_env = setHUG deps hsc_env
-          in wrapAction wrapper lcl_hsc_env $ do
+          in wrapAction lcl_hsc_env $ do
             res <- upsweep_inst lcl_hsc_env msg k n uid iu
             cleanCurrentModuleTempFilesMaybe (hsc_logger hsc_env) (hsc_tmpfs hsc_env) (hsc_dflags hsc_env)
             return res
@@ -2534,7 +2501,7 @@ executeCompileNode k n !old_hmi hug mrehydrate_mods mod = do
              hydrated_hsc_env
      -- Compile the module, locking with a semaphore to avoid too many modules
      -- being compiled at the same time leading to high memory usage.
-     wrapAction diag_wrapper lcl_hsc_env $ do
+     wrapAction lcl_hsc_env $ do
       res <- upsweep_mod lcl_hsc_env env_messager old_hmi mod k n
       cleanCurrentModuleTempFilesMaybe (hsc_logger hsc_env) (hsc_tmpfs hsc_env) lcl_dynflags
       return res)
@@ -2546,7 +2513,7 @@ executeCompileNode k n !old_hmi hug mrehydrate_mods mod = do
         -- compiling a signature requires an knot_var for that unit.
         -- If you remove this then a lot of backpack tests fail.
         HsigFile -> Just []
-        _        -> mrehydrate_mods
+        _ -> mrehydrate_mods
 
 {- Rehydration, see Note [Rehydrating Modules] -}
 
@@ -2865,55 +2832,34 @@ label_self thread_name = do
     CC.labelThread self_tid thread_name
 
 
-runPipelines :: WorkerLimit -> HscEnv -> (GhcMessage -> AnyGhcDiagnostic) -> Maybe Messager -> [MakeAction] -> IO ()
+runPipelines :: Int -> HscEnv -> Maybe Messager -> [MakeAction] -> IO ()
 -- Don't even initialise plugins if there are no pipelines
-runPipelines n_job hsc_env diag_wrapper mHscMessager all_pipelines = do
+runPipelines _ _ _ [] = return ()
+runPipelines n_job orig_hsc_env mHscMessager all_pipelines = do
   liftIO $ label_self "main --make thread"
+
+  plugins_hsc_env <- initializePlugins orig_hsc_env
   case n_job of
-    NumProcessorsLimit n | n <= 1 -> runSeqPipelines hsc_env diag_wrapper mHscMessager all_pipelines
-    _n -> runParPipelines n_job hsc_env diag_wrapper mHscMessager all_pipelines
+    1 -> runSeqPipelines plugins_hsc_env mHscMessager all_pipelines
+    _n -> runParPipelines n_job plugins_hsc_env mHscMessager all_pipelines
 
-runSeqPipelines :: HscEnv -> (GhcMessage -> AnyGhcDiagnostic) -> Maybe Messager -> [MakeAction] -> IO ()
-runSeqPipelines plugin_hsc_env diag_wrapper mHscMessager all_pipelines =
+runSeqPipelines :: HscEnv -> Maybe Messager -> [MakeAction] -> IO ()
+runSeqPipelines plugin_hsc_env mHscMessager all_pipelines =
   let env = MakeEnv { hsc_env = plugin_hsc_env
                     , withLogger = \_ k -> k id
                     , compile_sem = AbstractSem (return ()) (return ())
                     , env_messager = mHscMessager
-                    , diag_wrapper = diag_wrapper
                     }
-  in runAllPipelines (NumProcessorsLimit 1) env all_pipelines
+  in runAllPipelines 1 env all_pipelines
 
-runNjobsAbstractSem :: Int -> (AbstractSem -> IO a) -> IO a
-runNjobsAbstractSem n_jobs action = do
-  compile_sem <- newQSem n_jobs
-  n_capabilities <- getNumCapabilities
-  n_cpus <- getNumProcessors
-  let
-    asem = AbstractSem (waitQSem compile_sem) (signalQSem compile_sem)
-    set_num_caps n = unless (n_capabilities /= 1) $ setNumCapabilities n
-    updNumCapabilities =  do
-      -- Setting number of capabilities more than
-      -- CPU count usually leads to high userspace
-      -- lock contention. #9221
-      set_num_caps $ min n_jobs n_cpus
-    resetNumCapabilities = set_num_caps n_capabilities
-  MC.bracket_ updNumCapabilities resetNumCapabilities $ action asem
-
-runWorkerLimit :: WorkerLimit -> (AbstractSem -> IO a) -> IO a
-runWorkerLimit worker_limit action = case worker_limit of
-    NumProcessorsLimit n_jobs ->
-      runNjobsAbstractSem n_jobs action
-    JSemLimit sem ->
-      runJSemAbstractSem sem action
 
 -- | Build and run a pipeline
-runParPipelines :: WorkerLimit -- ^ How to limit work parallelism
-             -> HscEnv         -- ^ The basic HscEnv which is augmented with specific info for each module
-             -> (GhcMessage -> AnyGhcDiagnostic)
+runParPipelines :: Int              -- ^ How many capabilities to use
+             -> HscEnv           -- ^ The basic HscEnv which is augmented with specific info for each module
              -> Maybe Messager   -- ^ Optional custom messager to use to report progress
              -> [MakeAction]  -- ^ The build plan for all the module nodes
              -> IO ()
-runParPipelines worker_limit plugin_hsc_env diag_wrapper mHscMessager all_pipelines = do
+runParPipelines n_jobs plugin_hsc_env mHscMessager all_pipelines = do
 
 
   -- A variable which we write to when an error has happened and we have to tell the
@@ -2923,24 +2869,39 @@ runParPipelines worker_limit plugin_hsc_env diag_wrapper mHscMessager all_pipeli
   -- will add it's LogQueue into this queue.
   log_queue_queue_var <- newTVarIO newLogQueueQueue
   -- Thread which coordinates the printing of logs
-  wait_log_thread <- logThread (hsc_logger plugin_hsc_env) stopped_var log_queue_queue_var
+  wait_log_thread <- logThread n_jobs (length all_pipelines) (hsc_logger plugin_hsc_env) stopped_var log_queue_queue_var
 
 
   -- Make the logger thread-safe, in case there is some output which isn't sent via the LogQueue.
   thread_safe_logger <- liftIO $ makeThreadSafe (hsc_logger plugin_hsc_env)
   let thread_safe_hsc_env = plugin_hsc_env { hsc_logger = thread_safe_logger }
 
-  runWorkerLimit worker_limit $ \abstract_sem -> do
-    let env = MakeEnv { hsc_env = thread_safe_hsc_env
-                      , withLogger = withParLog log_queue_queue_var
-                      , compile_sem = abstract_sem
-                      , env_messager = mHscMessager
-                      , diag_wrapper = diag_wrapper
-                      }
+  let updNumCapabilities = liftIO $ do
+          n_capabilities <- getNumCapabilities
+          n_cpus <- getNumProcessors
+          -- Setting number of capabilities more than
+          -- CPU count usually leads to high userspace
+          -- lock contention. #9221
+          let n_caps = min n_jobs n_cpus
+          unless (n_capabilities /= 1) $ setNumCapabilities n_caps
+          return n_capabilities
+
+  let resetNumCapabilities orig_n = do
+          liftIO $ setNumCapabilities orig_n
+          atomically $ writeTVar stopped_var True
+          wait_log_thread
+
+  compile_sem <- newQSem n_jobs
+  let abstract_sem = AbstractSem (waitQSem compile_sem) (signalQSem compile_sem)
     -- Reset the number of capabilities once the upsweep ends.
-    runAllPipelines worker_limit env all_pipelines
-    atomically $ writeTVar stopped_var True
-    wait_log_thread
+  let env = MakeEnv { hsc_env = thread_safe_hsc_env
+                    , withLogger = withParLog log_queue_queue_var
+                    , compile_sem = abstract_sem
+                    , env_messager = mHscMessager
+                    }
+
+  MC.bracket updNumCapabilities resetNumCapabilities $ \_ ->
+    runAllPipelines n_jobs env all_pipelines
 
 withLocalTmpFS :: RunMakeM a -> RunMakeM a
 withLocalTmpFS act = do
@@ -2957,11 +2918,10 @@ withLocalTmpFS act = do
   MC.bracket initialiser finaliser $ \lcl_hsc_env -> local (\env -> env { hsc_env = lcl_hsc_env}) act
 
 -- | Run the given actions and then wait for them all to finish.
-runAllPipelines :: WorkerLimit -> MakeEnv -> [MakeAction] -> IO ()
-runAllPipelines worker_limit env acts = do
-  let single_worker = isWorkerLimitSequential worker_limit
-      spawn_actions :: IO [ThreadId]
-      spawn_actions = if single_worker
+runAllPipelines :: Int -> MakeEnv -> [MakeAction] -> IO ()
+runAllPipelines n_jobs env acts = do
+  let spawn_actions :: IO [ThreadId]
+      spawn_actions = if n_jobs == 1
         then (:[]) <$> (forkIOWithUnmask $ \unmask -> void $ runLoop (\io -> io unmask) env acts)
         else runLoop forkIOWithUnmask env acts
 
